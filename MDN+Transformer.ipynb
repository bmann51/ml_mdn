{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "eReFPV9T_Swu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "with open(\"bird.ndjson\", 'rb') as f_in:\n",
        "    with gzip.open(\"bird.ndjson.gz\", 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w1psEh9davFM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "293LnkhQ_V08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Transformer Decoder + MDN for Stroke Completion (predicting remaining points)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gzip, json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Dataset (return split: first_half, second_half) ----------\n",
        "\n",
        "def filter_good_drawings(drawing, min_strokes=3, max_strokes=10, min_points=30, max_points=150, closed_threshold=0.15):\n",
        "    total_points = 0\n",
        "    coords = []\n",
        "    for stroke in drawing:\n",
        "        xs, ys = stroke\n",
        "        if len(xs) == 0: continue\n",
        "        for x, y in zip(xs, ys):\n",
        "            coords.append((x, y))\n",
        "        total_points += len(xs)\n",
        "\n",
        "    if not (min_points <= total_points <= max_points):\n",
        "        return False\n",
        "    if not (min_strokes <= len(drawing) <= max_strokes):\n",
        "        return False\n",
        "\n",
        "    # Check for closure: distance between start and end\n",
        "    start = np.array(coords[0])\n",
        "    end = np.array(coords[-1])\n",
        "    dist = np.linalg.norm(start - end)\n",
        "    diagonal = np.linalg.norm(np.ptp(np.array(coords), axis=0)) + 1e-5\n",
        "    closedness = dist / diagonal\n",
        "    if closedness > closed_threshold:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "class QuickDrawDataset(Dataset):\n",
        "    def __init__(self, file_path, max_len=200, limit=5000):\n",
        "        self.samples = []\n",
        "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                drawing = data['drawing']\n",
        "                if not data.get('recognized', False):\n",
        "                    continue\n",
        "                if not filter_good_drawings(drawing):\n",
        "                    continue\n",
        "                drawing = data['drawing']\n",
        "                seq = []\n",
        "                prev_x, prev_y = 0, 0\n",
        "                for stroke in drawing:\n",
        "                    xs, ys = stroke\n",
        "                    for i in range(len(xs)):\n",
        "                        dx = xs[i] - prev_x\n",
        "                        dy = ys[i] - prev_y\n",
        "                        pen = 0 if i < len(xs) - 1 else 1\n",
        "                        seq.append([dx, dy, pen])\n",
        "                        prev_x, prev_y = xs[i], ys[i]\n",
        "                if 20 < len(seq) < max_len:\n",
        "                    seq = np.array(seq, dtype=np.float32)\n",
        "                    seq[:, 0] = (seq[:, 0] - np.mean(seq[:, 0])) / (np.std(seq[:, 0]) + 1e-5)\n",
        "                    seq[:, 1] = (seq[:, 1] - np.mean(seq[:, 1])) / (np.std(seq[:, 1]) + 1e-5)\n",
        "                    mid = len(seq) // 2\n",
        "                    first = np.zeros((max_len, 3), dtype=np.float32)\n",
        "                    second = np.zeros((max_len, 3), dtype=np.float32)\n",
        "                    first[:mid] = seq[:mid]\n",
        "                    second[:len(seq)-mid] = seq[mid:]\n",
        "                    self.samples.append((first, second))\n",
        "                    if len(self.samples) >= limit:\n",
        "                        break\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.samples[idx][0]), torch.tensor(self.samples[idx][1])\n",
        "\n",
        "# ---------- Positional Encoding ----------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# ---------- Transformer Decoder + MDN ----------\n",
        "class TransformerDecoderMDN(nn.Module):\n",
        "    def __init__(self, input_dim=3, d_model=128, nhead=4, num_layers=4, ff_dim=256, num_mixtures=20):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=ff_dim)\n",
        "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.output_layer = nn.Linear(d_model, num_mixtures * 6 + 1)\n",
        "        self.num_mixtures = num_mixtures\n",
        "\n",
        "    def forward(self, memory, tgt):\n",
        "        B, T, _ = tgt.size()\n",
        "        tgt_emb = self.input_proj(tgt)\n",
        "        tgt_emb = self.pos_encoder(tgt_emb)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(tgt.device)\n",
        "        out = self.transformer(tgt=tgt_emb.transpose(0, 1), memory=memory.transpose(0, 1), tgt_mask=tgt_mask)\n",
        "        return self.output_layer(out.transpose(0, 1))\n",
        "\n",
        "# ---------- MDN helper functions ----------\n",
        "\n",
        "\n",
        "\n",
        "def split_mdn_params(output, num_mixtures):\n",
        "    eos_hat = output[:, :, 0:1]\n",
        "    rest = output[:, :, 1:]\n",
        "    pi_hat, mu1, mu2, sigma1, sigma2, rho = torch.chunk(rest, 6, dim=-1)\n",
        "    pi = F.softmax(pi_hat, dim=-1)\n",
        "    sigma1 = torch.clamp(torch.exp(sigma1), 1e-3, 1.0)\n",
        "    sigma2 = torch.clamp(torch.exp(sigma2), 1e-3, 1.0)\n",
        "    rho = torch.tanh(rho)\n",
        "    eos = torch.sigmoid(eos_hat)\n",
        "    return pi, mu1, mu2, sigma1, sigma2, rho, eos\n",
        "\n",
        "def mdn_loss(y_true, mdn_params, pen_loss_weight=0.05, sigma_weight=0.01, entropy_weight=0.01):\n",
        "    x1 = y_true[:, :, 0:1]\n",
        "    x2 = y_true[:, :, 1:2]\n",
        "    eos_true = y_true[:, :, 2:3]\n",
        "    pi, mu1, mu2, sigma1, sigma2, rho, eos = mdn_params\n",
        "\n",
        "    norm1 = (x1 - mu1) / sigma1\n",
        "    norm2 = (x2 - mu2) / sigma2\n",
        "    z = norm1**2 + norm2**2 - 2 * rho * norm1 * norm2\n",
        "    denom = 2 * (1 - rho**2)\n",
        "    exponent = -z / denom\n",
        "    coef = 1 / (2 * np.pi * sigma1 * sigma2 * torch.sqrt(1 - rho**2))\n",
        "    gauss = coef * torch.exp(exponent)\n",
        "    prob = torch.sum(pi * gauss, dim=-1, keepdim=True)\n",
        "\n",
        "    pen_loss = eos_true * torch.log(eos + 1e-8) + (1 - eos_true) * torch.log(1 - eos + 1e-8)\n",
        "    loss_seq = -torch.log(prob + 1e-8) - pen_loss_weight * pen_loss\n",
        "\n",
        "    sigma_penalty = sigma_weight * (torch.mean(sigma1) + torch.mean(sigma2))\n",
        "    entropy = -torch.sum(pi * torch.log(pi + 1e-8), dim=-1, keepdim=True)\n",
        "    entropy_penalty = -entropy_weight * torch.mean(entropy)\n",
        "\n",
        "    mu_x_mean = torch.sum(pi * mu1, dim=-1)\n",
        "    mu_y_mean = torch.sum(pi * mu2, dim=-1)\n",
        "    true_x = x1.squeeze(-1)\n",
        "    true_y = x2.squeeze(-1)\n",
        "    l2_loss = F.mse_loss(mu_x_mean, true_x) + F.mse_loss(mu_y_mean, true_y)\n",
        "\n",
        "    final_loss = torch.mean(loss_seq) + sigma_penalty + entropy_penalty + 0.05 * l2_loss\n",
        "    return final_loss\n",
        "\n",
        "\n",
        "# ---------- Train loop ----------\n",
        "\n",
        "def sample_completion(model, memory, max_len=100):\n",
        "    model.eval()\n",
        "    device = memory.device\n",
        "    x, y = 0.0, 0.0\n",
        "    prev = torch.tensor([[[0.0, 0.0, 1.0]]], dtype=torch.float32).to(device)\n",
        "    seq = [prev.squeeze(0)]\n",
        "    points = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_seq = torch.stack(seq, dim=1).to(device)\n",
        "        output = model(memory, tgt_seq)\n",
        "        pi, mu1, mu2, sigma1, sigma2, rho, eos = split_mdn_params(output, model.num_mixtures)\n",
        "\n",
        "        idx = np.random.choice(model.num_mixtures, p=pi[0, -1].detach().cpu().numpy())\n",
        "        mean = [mu1[0, -1, idx].item(), mu2[0, -1, idx].item()]\n",
        "        cov = [[sigma1[0, -1, idx].item()**2, rho[0, -1, idx].item() * sigma1[0, -1, idx].item() * sigma2[0, -1, idx].item()],\n",
        "               [rho[0, -1, idx].item() * sigma1[0, -1, idx].item() * sigma2[0, -1, idx].item(), sigma2[0, -1, idx].item()**2]]\n",
        "        dx, dy = np.random.multivariate_normal(mean, cov)\n",
        "        eos_sample = np.random.binomial(1, eos[0, -1, 0].item())\n",
        "        x += dx\n",
        "        y += dy\n",
        "        points.append(None if eos_sample else (x, y))\n",
        "        seq.append(torch.tensor([[dx, dy, eos_sample]], dtype=torch.float32).to(device))\n",
        "        if eos_sample and len(seq) > 10:\n",
        "            break\n",
        "\n",
        "    return points\n",
        "\n",
        "def stroke_to_points(seq):\n",
        "    \"\"\"Convert [dx, dy, pen] sequence (Tensor or numpy) to absolute (x, y) points\"\"\"\n",
        "    if torch.is_tensor(seq):\n",
        "        seq = seq.detach().cpu().numpy()\n",
        "    x, y = 0.0, 0.0\n",
        "    points = []\n",
        "    for dx, dy, p in seq:\n",
        "        x += dx\n",
        "        y += dy\n",
        "        if p < 0.5:\n",
        "            points.append((x, y))\n",
        "        else:\n",
        "            points.append(None)\n",
        "    return points\n",
        "\n",
        "\n",
        "def plot_stroke(points, ax, title=\"\"):\n",
        "    x, y = [], []\n",
        "    for pt in points:\n",
        "        if pt is None:\n",
        "            if x and y:\n",
        "                ax.plot(x, y, color='black')\n",
        "                x, y = [], []\n",
        "        else:\n",
        "            x.append(pt[0])\n",
        "            y.append(pt[1])\n",
        "    if x and y:\n",
        "        ax.plot(x, y, color='black')\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"equal\")\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "def visualize_completion_sample(model, dataset):\n",
        "    import random\n",
        "    model.eval()\n",
        "    i = random.randint(0, len(dataset)-1)\n",
        "    first_half, second_half = dataset[i]\n",
        "    first_half = first_half.unsqueeze(0).to(next(model.parameters()).device)\n",
        "\n",
        "    # Project first_half to memory\n",
        "    memory = model.input_proj(first_half)\n",
        "    memory = model.pos_encoder(memory)\n",
        "\n",
        "    # Sample predicted second half\n",
        "    pred_points = sample_completion(model, memory)\n",
        "\n",
        "    # Convert ground truth, input, and prediction to full stroke sequences\n",
        "    input_points = stroke_to_points(first_half[0].detach().cpu())\n",
        "    gt_points = stroke_to_points(torch.cat([\n",
        "    first_half[0].detach().cpu(),\n",
        "    second_half.detach().cpu()\n",
        "], dim=0))\n",
        "\n",
        "    # Append predicted points to the input path\n",
        "    completed_points = []\n",
        "    for pt in input_points:\n",
        "        completed_points.append(pt)\n",
        "    for pt in pred_points:\n",
        "        completed_points.append(pt)\n",
        "\n",
        "    # Plot\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    plot_stroke(gt_points, axs[0], \"Ground Truth\")\n",
        "    plot_stroke(input_points, axs[1], \"Input (First Half)\")\n",
        "    plot_stroke(completed_points, axs[2], \"Model Completion (Full)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = QuickDrawDataset(\"bird.ndjson.gz\")\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    model = TransformerDecoderMDN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        for first_half, second_half in loader:\n",
        "            first_half, second_half = first_half.to(device), second_half.to(device)\n",
        "            memory = model.input_proj(first_half)\n",
        "            memory = model.pos_encoder(memory)\n",
        "            output = model(memory, second_half[:, :-1, :])\n",
        "            loss = mdn_loss(second_half[:, 1:, :], split_mdn_params(output, model.num_mixtures))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "        visualize_completion_sample(model, dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "dWUTlYh99Ohd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "HrhZNSAc_ZoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "test_indices = torch.load(\"test_indices.pt\")\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "def visualize_mdn_on_test(model, test_dataset):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    for i in range(len(test_dataset)):\n",
        "        first_half, second_half = test_dataset[i]\n",
        "        first_half = first_half.unsqueeze(0).to(device)\n",
        "\n",
        "        # encode memory\n",
        "        memory = model.input_proj(first_half)\n",
        "        memory = model.pos_encoder(memory)\n",
        "\n",
        "        # use your existing MDN sampling function\n",
        "        mdn_points = sample_completion(model, memory)\n",
        "\n",
        "        # convert ground truth and input\n",
        "        input_points = stroke_to_points(first_half[0])\n",
        "        gt_points = stroke_to_points(torch.cat([first_half[0].cpu(), second_half.cpu()], dim=0))\n",
        "        completed_points = input_points + mdn_points\n",
        "\n",
        "        # plot\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "        plot_stroke(gt_points, axs[0], \"Ground Truth\")\n",
        "        plot_stroke(input_points, axs[1], \"Input (First Half)\")\n",
        "        plot_stroke(completed_points, axs[2], \"MDN Completion\")\n",
        "        plt.suptitle(f\"Sample {i+1}\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "visualize_mdn_on_test(model, test_dataset)\n"
      ],
      "metadata": {
        "id": "SC29LWM6sIk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}