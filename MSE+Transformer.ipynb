{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "KEUrfLwn_ply"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0dOd8SljsNlQ"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "with open(\"bird.ndjson\", 'rb') as f_in:\n",
        "    with gzip.open(\"bird.ndjson.gz\", 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "Gt5SFXSx_tWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Clean MSE-based Transformer Decoder for Stroke Completion\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gzip, json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Dataset (first half, second half) ----------\n",
        "\n",
        "def filter_good_drawings(drawing, min_strokes=3, max_strokes=10, min_points=30, max_points=150, closed_threshold=0.15):\n",
        "    total_points = 0\n",
        "    coords = []\n",
        "    for stroke in drawing:\n",
        "        xs, ys = stroke\n",
        "        if len(xs) == 0: continue\n",
        "        for x, y in zip(xs, ys):\n",
        "            coords.append((x, y))\n",
        "        total_points += len(xs)\n",
        "    if not (min_points <= total_points <= max_points): return False\n",
        "    if not (min_strokes <= len(drawing) <= max_strokes): return False\n",
        "    start = np.array(coords[0])\n",
        "    end = np.array(coords[-1])\n",
        "    dist = np.linalg.norm(start - end)\n",
        "    diagonal = np.linalg.norm(np.ptp(np.array(coords), axis=0)) + 1e-5\n",
        "    if (dist / diagonal) > closed_threshold: return False\n",
        "    return True\n",
        "\n",
        "class QuickDrawDataset(Dataset):\n",
        "    def __init__(self, file_path, max_len=200, limit=5000):\n",
        "        self.samples = []\n",
        "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                if not data.get('recognized', False): continue\n",
        "                if not filter_good_drawings(data['drawing']): continue\n",
        "                drawing = data['drawing']\n",
        "                seq, prev_x, prev_y = [], 0, 0\n",
        "                for stroke in drawing:\n",
        "                    xs, ys = stroke\n",
        "                    for i in range(len(xs)):\n",
        "                        dx, dy = xs[i] - prev_x, ys[i] - prev_y\n",
        "                        pen = 0 if i < len(xs)-1 else 1\n",
        "                        seq.append([dx, dy, pen])\n",
        "                        prev_x, prev_y = xs[i], ys[i]\n",
        "                if 20 < len(seq) < max_len:\n",
        "                    seq = np.array(seq, dtype=np.float32)\n",
        "                    seq[:, 0] = (seq[:, 0] - np.mean(seq[:, 0])) / (np.std(seq[:, 0]) + 1e-5)\n",
        "                    seq[:, 1] = (seq[:, 1] - np.mean(seq[:, 1])) / (np.std(seq[:, 1]) + 1e-5)\n",
        "                    mid = len(seq) // 2\n",
        "                    first = np.zeros((max_len, 3), dtype=np.float32)\n",
        "                    second = np.zeros((max_len, 3), dtype=np.float32)\n",
        "                    first[:mid] = seq[:mid]\n",
        "                    second[:len(seq)-mid] = seq[mid:]\n",
        "                    self.samples.append((first, second))\n",
        "                    if len(self.samples) >= limit: break\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.samples[idx][0]), torch.tensor(self.samples[idx][1])\n",
        "\n",
        "# ---------- Positional Encoding ----------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "# ---------- Transformer Decoder (MSE version) ----------\n",
        "class TransformerDecoderMSE(nn.Module):\n",
        "    def __init__(self, input_dim=3, d_model=128, nhead=4, num_layers=4, ff_dim=256):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, ff_dim)\n",
        "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.output_layer = nn.Linear(d_model, 3)  # dx, dy, pen\n",
        "\n",
        "    def forward(self, memory, tgt):\n",
        "        B, T, _ = tgt.size()\n",
        "        tgt_emb = self.input_proj(tgt)\n",
        "        tgt_emb = self.pos_encoder(tgt_emb)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(tgt.device)\n",
        "        out = self.transformer(tgt_emb.transpose(0,1), memory.transpose(0,1), tgt_mask=tgt_mask)\n",
        "        return self.output_layer(out.transpose(0,1))\n",
        "\n",
        "# ---------- MSE Loss ----------\n",
        "def mse_loss(pred, target, pen_weight=0.01):\n",
        "    l2_loss = F.mse_loss(pred[:, :, :2], target[:, :, :2])\n",
        "    pen_loss = F.binary_cross_entropy_with_logits(pred[:, :, 2], target[:, :, 2])\n",
        "    return l2_loss + pen_weight * pen_loss\n",
        "\n",
        "# ---------- Plotting ----------\n",
        "def stroke_to_points(seq):\n",
        "    if torch.is_tensor(seq): seq = seq.cpu().numpy()\n",
        "    x, y, points = 0.0, 0.0, []\n",
        "    for dx, dy, p in seq:\n",
        "        x += dx\n",
        "        y += dy\n",
        "        points.append(None if p >= 0.5 else (x, y))\n",
        "    return points\n",
        "\n",
        "def plot_stroke(points, ax, title=\"\"):\n",
        "    x, y = [], []\n",
        "    for pt in points:\n",
        "        if pt is None:\n",
        "            if x: ax.plot(x, y, color='black'); x, y = [], []\n",
        "        else:\n",
        "            x.append(pt[0]); y.append(pt[1])\n",
        "    if x: ax.plot(x, y, color='black')\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"equal\")\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "def visualize_completion_sample(model, dataset):\n",
        "    import random\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    first_half, second_half = dataset[random.randint(0, len(dataset)-1)]\n",
        "    first_half = first_half.unsqueeze(0).to(device)\n",
        "    second_half = second_half.unsqueeze(0).to(device)\n",
        "    memory = model.input_proj(first_half)\n",
        "    memory = model.pos_encoder(memory)\n",
        "    pred = model(memory, second_half[:, :-1, :])\n",
        "    pred_points = stroke_to_points(pred[0].detach())\n",
        "    input_points = stroke_to_points(first_half[0])\n",
        "    gt_points = stroke_to_points(torch.cat([first_half[0], second_half[0]], dim=0))\n",
        "    completed_points = input_points + pred_points\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    plot_stroke(gt_points, axs[0], \"Ground Truth\")\n",
        "    plot_stroke(input_points, axs[1], \"Input (First Half)\")\n",
        "    plot_stroke(completed_points, axs[2], \"Model Completion\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------- Train Loop ----------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = QuickDrawDataset(\"bird.ndjson.gz\")\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    model = TransformerDecoderMSE().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        for first_half, second_half in loader:\n",
        "            first_half, second_half = first_half.to(device), second_half.to(device)\n",
        "            memory = model.input_proj(first_half)\n",
        "            memory = model.pos_encoder(memory)\n",
        "            output = model(memory, second_half[:, :-1, :])\n",
        "            loss = mse_loss(output, second_half[:, 1:, :])\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "        visualize_completion_sample(model, dataset)"
      ],
      "metadata": {
        "id": "Jh8NW-tdsT_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 假设你用的是 QuickDrawDataset(\"bird.ndjson.gz\")\n",
        "from torch.utils.data import Subset\n",
        "import torch\n",
        "\n",
        "# 固定随机种子保证每次选的一样\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 加载完整数据集\n",
        "full_dataset = QuickDrawDataset(\"bird.ndjson.gz\")\n",
        "\n",
        "# 随机抽取 100 个样本\n",
        "indices = torch.randperm(len(full_dataset))[:100]\n",
        "test_subset = Subset(full_dataset, indices)\n",
        "\n",
        "# 保存到本地（推荐用 .pt 文件）\n",
        "torch.save(indices, \"test_indices.pt\")\n"
      ],
      "metadata": {
        "id": "j7TlOtxBzUWL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "# 加载完整数据集\n",
        "full_dataset = QuickDrawDataset(\"bird.ndjson.gz\")\n",
        "\n",
        "# 加载固定 index（确保两个模型都用一样的 subset）\n",
        "test_indices = torch.load(\"test_indices.pt\")\n",
        "test_dataset = Subset(full_dataset, test_indices)\n"
      ],
      "metadata": {
        "id": "lpAl7O2czVIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "VwkNUzEh_vvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Clean MSE-based Transformer Decoder for Stroke Completion\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gzip, json\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Dataset (first half, second half) ----------\n",
        "\n",
        "def filter_good_drawings(drawing, min_strokes=3, max_strokes=10, min_points=30, max_points=150, closed_threshold=0.15):\n",
        "    total_points = 0\n",
        "    coords = []\n",
        "    for stroke in drawing:\n",
        "        xs, ys = stroke\n",
        "        if len(xs) == 0: continue\n",
        "        for x, y in zip(xs, ys):\n",
        "            coords.append((x, y))\n",
        "        total_points += len(xs)\n",
        "    if not (min_points <= total_points <= max_points): return False\n",
        "    if not (min_strokes <= len(drawing) <= max_strokes): return False\n",
        "    start = np.array(coords[0])\n",
        "    end = np.array(coords[-1])\n",
        "    dist = np.linalg.norm(start - end)\n",
        "    diagonal = np.linalg.norm(np.ptp(np.array(coords), axis=0)) + 1e-5\n",
        "    if (dist / diagonal) > closed_threshold: return False\n",
        "    return True\n",
        "\n",
        "class QuickDrawDataset(Dataset):\n",
        "    def __init__(self, file_path, max_len=200, limit=5000):\n",
        "        self.samples = []\n",
        "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                if not data.get('recognized', False): continue\n",
        "                if not filter_good_drawings(data['drawing']): continue\n",
        "                drawing = data['drawing']\n",
        "                seq, prev_x, prev_y = [], 0, 0\n",
        "                for stroke in drawing:\n",
        "                    xs, ys = stroke\n",
        "                    for i in range(len(xs)):\n",
        "                        dx, dy = xs[i] - prev_x, ys[i] - prev_y\n",
        "                        pen = 0 if i < len(xs)-1 else 1\n",
        "                        seq.append([dx, dy, pen])\n",
        "                        prev_x, prev_y = xs[i], ys[i]\n",
        "                if 20 < len(seq) < max_len:\n",
        "                    seq = np.array(seq, dtype=np.float32)\n",
        "                    seq[:, 0] = (seq[:, 0] - np.mean(seq[:, 0])) / (np.std(seq[:, 0]) + 1e-5)\n",
        "                    seq[:, 1] = (seq[:, 1] - np.mean(seq[:, 1])) / (np.std(seq[:, 1]) + 1e-5)\n",
        "                    mid = len(seq) // 2\n",
        "                    first = np.zeros((max_len, 3), dtype=np.float32)\n",
        "                    second = np.zeros((max_len, 3), dtype=np.float32)\n",
        "                    first[:mid] = seq[:mid]\n",
        "                    second[:len(seq)-mid] = seq[mid:]\n",
        "                    self.samples.append((first, second))\n",
        "                    if len(self.samples) >= limit: break\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.samples[idx][0]), torch.tensor(self.samples[idx][1])\n",
        "\n",
        "# ---------- Positional Encoding ----------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "# ---------- Transformer Decoder (MSE version) ----------\n",
        "class TransformerDecoderMSE(nn.Module):\n",
        "    def __init__(self, input_dim=3, d_model=128, nhead=4, num_layers=4, ff_dim=256):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, ff_dim)\n",
        "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.output_layer = nn.Linear(d_model, 3)  # dx, dy, pen\n",
        "\n",
        "    def forward(self, memory, tgt):\n",
        "        B, T, _ = tgt.size()\n",
        "        tgt_emb = self.input_proj(tgt)\n",
        "        tgt_emb = self.pos_encoder(tgt_emb)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(tgt.device)\n",
        "        out = self.transformer(tgt_emb.transpose(0,1), memory.transpose(0,1), tgt_mask=tgt_mask)\n",
        "        return self.output_layer(out.transpose(0,1))\n",
        "\n",
        "# ---------- MSE Loss ----------\n",
        "def mse_loss(pred, target, pen_weight=0.01):\n",
        "    l2_loss = F.mse_loss(pred[:, :, :2], target[:, :, :2])\n",
        "    pen_loss = F.binary_cross_entropy_with_logits(pred[:, :, 2], target[:, :, 2])\n",
        "    return l2_loss + pen_weight * pen_loss\n",
        "\n",
        "# ---------- Plotting ----------\n",
        "def stroke_to_points(seq):\n",
        "    if torch.is_tensor(seq): seq = seq.cpu().numpy()\n",
        "    x, y, points = 0.0, 0.0, []\n",
        "    for dx, dy, p in seq:\n",
        "        x += dx\n",
        "        y += dy\n",
        "        points.append(None if p >= 0.5 else (x, y))\n",
        "    return points\n",
        "\n",
        "def plot_stroke(points, ax, title=\"\"):\n",
        "    x, y = [], []\n",
        "    for pt in points:\n",
        "        if pt is None:\n",
        "            if x: ax.plot(x, y, color='black'); x, y = [], []\n",
        "        else:\n",
        "            x.append(pt[0]); y.append(pt[1])\n",
        "    if x: ax.plot(x, y, color='black')\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"equal\")\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "def visualize_mdn_vs_mse(model_mdn, model_mse, dataset):\n",
        "    device = next(model_mdn.parameters()).device\n",
        "    for i in range(len(dataset)):\n",
        "        first_half, second_half = dataset[i]\n",
        "        first_half = first_half.unsqueeze(0).to(device)\n",
        "        second_half = second_half.unsqueeze(0).to(device)\n",
        "\n",
        "        memory = model_mdn.input_proj(first_half)\n",
        "        memory = model_mdn.pos_encoder(memory)\n",
        "\n",
        "        # MDN sampling (you need to define this separately)\n",
        "        mdn_points = sample_completion(model_mdn, memory)\n",
        "\n",
        "        # MSE deterministic\n",
        "        mse_pred = model_mse(memory, second_half[:, :-1, :])\n",
        "        mse_points = stroke_to_points(mse_pred[0].detach())\n",
        "\n",
        "        input_points = stroke_to_points(first_half[0])\n",
        "        gt_points = stroke_to_points(torch.cat([first_half[0], second_half[0]], dim=0))\n",
        "\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "        plot_stroke(gt_points, axs[0], \"Ground Truth\")\n",
        "        plot_stroke(input_points, axs[1], \"Input\")\n",
        "        plot_stroke(mdn_points, axs[2], \"MDN Completion\")\n",
        "        plot_stroke(mse_points, axs[3], \"MSE Completion\")\n",
        "        plt.tight_layout()\n",
        "        print(f\"Sample {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "# (sample_completion should be implemented if you're using this block)\n",
        "# (you can now call visualize_mdn_vs_mse(model_mdn, model_mse, test_dataset))\n"
      ],
      "metadata": {
        "id": "ZKsZ6LxS2OQj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_mse_on_test(model, test_dataset):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    for i in range(len(test_dataset)):\n",
        "        first_half, second_half = test_dataset[i]\n",
        "        first_half = first_half.unsqueeze(0).to(device)\n",
        "        second_half = second_half.unsqueeze(0).to(device)\n",
        "\n",
        "        memory = model.input_proj(first_half)\n",
        "        memory = model.pos_encoder(memory)\n",
        "\n",
        "        # Predict second half deterministically\n",
        "        pred = model(memory, second_half[:, :-1, :])\n",
        "        pred_points = stroke_to_points(pred[0].detach())\n",
        "        input_points = stroke_to_points(first_half[0])\n",
        "        gt_points = stroke_to_points(torch.cat([first_half[0], second_half[0]], dim=0))\n",
        "        completed_points = input_points + pred_points\n",
        "\n",
        "        # Plot\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "        plot_stroke(gt_points, axs[0], \"Ground Truth\")\n",
        "        plot_stroke(input_points, axs[1], \"Input (First Half)\")\n",
        "        plot_stroke(completed_points, axs[2], \"MSE Completion\")\n",
        "        plt.suptitle(f\"Sample {i+1}\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# 如果你还没 split，可以先选一个固定的测试集\n",
        "torch.manual_seed(99)\n",
        "test_indices = torch.randperm(len(dataset))[:100]\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "torch.save(test_indices, \"test_indices.pt\")\n",
        "# 然后可视化所有\n",
        "visualize_mse_on_test(model, test_dataset)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2PF5yxil2RoN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}